import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
import numpy as np
import os  # æ–°å¢ž: ç”¨æ–¼è™•ç†è·¯å¾‘

# å°Žå…¥æˆ‘å€‘è‡ªå·±å¯«çš„æ¨¡çµ„
from features.alignment import synthesize_mtf_data
from features.labeling import apply_triple_barrier
from features.preprocess import prepare_features
from models.tcn_core import ParallelTCNAlphaHunter
from utils.loss import FocalLoss, calculate_mcc
from data.dataset import CryptoTimeSeriesDataset

# --- è¨­å®šåƒæ•¸ (Hyperparameters) ---
CONFIG = {
    'seq_len': 60,  # Benchmark: 30, 40, 60, 80
    'norm_method': 'z_score',  # Benchmark: 'z_score' vs 'log_return'
    'batch_size': 64,
    'epochs': 20,
    'learning_rate': 1e-3,
    'atr_period': 14,
    'horizon': 60,  # Triple Barrier çš„æ™‚é–“ç‰†
    'pt_mul': 2.0,  # æ­¢ç›ˆå¯¬åº¦
    'sl_mul': 2.0,  # æ­¢æå¯¬åº¦
}


def load_and_clean_data(filepath):
    """
    å°ˆé–€è®€å–ç”¨æˆ¶æ ¼å¼çš„ CSV æª”æ¡ˆ
    æ ¼å¼: Open Time, Open, High, Low, Close, Volume
    """
    print(f"ðŸ“„ è®€å–æª”æ¡ˆ: {filepath}")
    # è®€å– CSV
    df = pd.read_csv(filepath)

    # 1. é‡æ–°å‘½åæ¬„ä½ (è½‰ç‚ºå°å¯«ä»¥ç¬¦åˆç³»çµ±è®Šæ•¸)
    rename_map = {
        'Open Time': 'datetime',
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Close': 'close',
        'Volume': 'volume'
    }
    # å®¹éŒ¯è™•ç†ï¼šæœ‰äº›æ•¸æ“šå¯èƒ½å·²ç¶“æ˜¯å°å¯«ï¼Œé€™è£¡åšå€‹æª¢æŸ¥
    current_cols = df.columns
    actual_rename = {k: v for k, v in rename_map.items() if k in current_cols}
    df = df.rename(columns=actual_rename)

    # 2. è™•ç†æ™‚é–“ç´¢å¼•
    if 'datetime' in df.columns:
        df['datetime'] = pd.to_datetime(df['datetime'])
        df.set_index('datetime', inplace=True)
    elif df.index.name != 'datetime':
        # å˜—è©¦å°‡ index è½‰ç‚º datetime (å¦‚æžœåŽŸæœ¬æ²’æœ‰ Open Time æ¬„ä½)
        df.index = pd.to_datetime(df.index)

    df.sort_index(inplace=True)

    # 3. ç¢ºä¿æ•¸å€¼åž‹æ…‹ (ç§»é™¤å¯èƒ½çš„å­—ä¸²)
    cols = ['open', 'high', 'low', 'close', 'volume']
    for c in cols:
        if c in df.columns:
            df[c] = df[c].astype(float)

    return df


def train_model():
    print(f"ðŸš€ å•Ÿå‹• Alpha Hunter è¨“ç·´ç¨‹åº...")
    print(f"âš™ï¸  è¨­å®š: Seq_Len={CONFIG['seq_len']}, Norm={CONFIG['norm_method']}")

    # 1. è¼‰å…¥æ•¸æ“š
    # Windows è·¯å¾‘è™•ç†: ä½¿ç”¨ os.path.join ç¢ºä¿ç›¸å®¹æ€§
    # æ³¨æ„ï¼šæˆ‘å€‘åªéœ€è¦è®€å– 1H æ•¸æ“šï¼Œfeatures/alignment.py æœƒå¹«æˆ‘å€‘ã€Œé›¶å»¶é²ã€åˆæˆ 4H å’Œ 1D
    # é€™æ¨£å¯ä»¥é¿å…ç›´æŽ¥è®€å– 1D æª”æ¡ˆé€ æˆçš„ã€Œå·çœ‹æœªä¾†ã€é¢¨éšª
    csv_path = os.path.join('data', 'raw', 'BTCUSDT_1H.csv')

    if os.path.exists(csv_path):
        df = load_and_clean_data(csv_path)
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(df)} ç­†æ•¸æ“š")
    else:
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°æª”æ¡ˆ {csv_path}")
        print("âš ï¸  åˆ‡æ›è‡³: ä½¿ç”¨éš¨æ©Ÿå‡æ•¸æ“šé€²è¡Œæ¸¬è©¦æ¨¡å¼...")
        dates = pd.date_range(start='2023-01-01', periods=2000, freq='1h')
        df = pd.DataFrame(np.random.random((2000, 5)) * 1000 + 20000,
                          index=dates,
                          columns=['open', 'high', 'low', 'close', 'volume'])

    # 2. ç‰¹å¾µå·¥ç¨‹ç®¡ç·š
    print("ðŸ”„ åŸ·è¡Œ Point-in-Time æ•¸æ“šå°é½Š (åˆæˆ 4H/1D)...")
    df_aligned = synthesize_mtf_data(df)

    print("ðŸ·ï¸  ç”Ÿæˆ Triple Barrier æ¨™ç±¤...")
    df_labeled = apply_triple_barrier(df_aligned,
                                      horizon=CONFIG['horizon'],
                                      atr_period=CONFIG['atr_period'],
                                      pt_mul=CONFIG['pt_mul'],
                                      sl_mul=CONFIG['sl_mul'])

    print("âš–ï¸  åŸ·è¡Œæ•¸æ“šæ¨™æº–åŒ–...")
    df_final = prepare_features(df_labeled,
                                method=CONFIG['norm_method'],
                                window=30)

    # æª¢æŸ¥æ˜¯å¦æœ‰æ¨™ç±¤ (å› ç‚º Triple Barrier åœ¨æœ€å¾Œ horizon æ ¹ K æ£’æœƒæ˜¯ NaN æˆ– 0)
    # æˆ‘å€‘ç§»é™¤ç„¡æ³•æ¨™è¨˜çš„å°¾éƒ¨æ•¸æ“š
    df_final = df_final.dropna()

    # 3. å»ºç«‹è³‡æ–™é›†èˆ‡ DataLoader
    # ç°¡å–®çš„æ™‚é–“åºåˆ—åˆ‡åˆ†: å‰ 80% è¨“ç·´, å¾Œ 20% é©—è­‰
    split_idx = int(len(df_final) * 0.8)
    train_df = df_final.iloc[:split_idx]
    val_df = df_final.iloc[split_idx:]

    print(f"ðŸ“Š è¨“ç·´é›†æ¨£æœ¬æ•¸: {len(train_df)}, é©—è­‰é›†æ¨£æœ¬æ•¸: {len(val_df)}")

    train_dataset = CryptoTimeSeriesDataset(train_df,
                                            seq_len=CONFIG['seq_len'])
    val_dataset = CryptoTimeSeriesDataset(val_df, seq_len=CONFIG['seq_len'])

    # å¦‚æžœæ•¸æ“šé‡å¤ªå°‘å°Žè‡´ dataset ç‚ºç©ºï¼Œåšå€‹ä¿è­·
    if len(train_dataset) == 0:
        print("âŒ éŒ¯èª¤: æœ‰æ•ˆæ•¸æ“šé‡ä¸è¶³ä»¥å»ºç«‹åºåˆ—ï¼Œè«‹æª¢æŸ¥ seq_len æˆ–æ•¸æ“šé•·åº¦ã€‚")
        return

    train_loader = DataLoader(train_dataset,
                              batch_size=CONFIG['batch_size'],
                              shuffle=True)
    val_loader = DataLoader(val_dataset,
                            batch_size=CONFIG['batch_size'],
                            shuffle=False)

    # 4. åˆå§‹åŒ–æ¨¡åž‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸ’» ä½¿ç”¨é‹ç®—è£ç½®: {device}")

    model = ParallelTCNAlphaHunter(input_features=5, num_classes=3).to(device)

    # 5. è¨­å®š Loss èˆ‡ Optimizer
    # æ ¹æ“šä½ çš„æ•¸æ“šï¼Œé€™è£¡çš„æ¬Šé‡å¯èƒ½éœ€è¦æ ¹æ“šå¯¦éš› Label åˆ†ä½ˆèª¿æ•´
    # ä½ å¯ä»¥å…ˆè·‘ä¸€æ¬¡ features/labeling.py çœ‹ä¸€ä¸‹åˆ†ä½ˆ
    focal_loss = FocalLoss(alpha=torch.tensor([0.5, 1.0, 1.0]).to(device),
                           gamma=2.0)
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])

    # 6. è¨“ç·´è¿´åœˆ
    for epoch in range(CONFIG['epochs']):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []

        for batch in train_loader:
            x_1h = batch['1h'].to(device)
            x_4h = batch['4h'].to(device)
            x_1d = batch['1d'].to(device)
            y = batch['label'].to(device)

            optimizer.zero_grad()
            logits = model(x_1h, x_4h, x_1d)
            loss = focal_loss(logits, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_preds.append(logits.detach())
            train_targets.append(y)

        # è¨ˆç®—è¨“ç·´é›† MCC
        if len(train_preds) > 0:
            train_preds = torch.cat(train_preds)
            train_targets = torch.cat(train_targets)
            train_mcc = calculate_mcc(train_preds, train_targets)
        else:
            train_mcc = 0

        # é©—è­‰
        model.eval()
        val_preds, val_targets = [], []
        with torch.no_grad():
            for batch in val_loader:
                x_1h = batch['1h'].to(device)
                x_4h = batch['4h'].to(device)
                x_1d = batch['1d'].to(device)
                y = batch['label'].to(device)

                logits = model(x_1h, x_4h, x_1d)
                val_preds.append(logits)
                val_targets.append(y)

        if len(val_preds) > 0:
            val_preds = torch.cat(val_preds)
            val_targets = torch.cat(val_targets)
            val_mcc = calculate_mcc(val_preds, val_targets)
        else:
            val_mcc = 0

        print(
            f"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {train_loss/len(train_loader):.4f} | Train MCC: {train_mcc:.3f} | Val MCC: {val_mcc:.3f}"
        )


if __name__ == "__main__":
    train_model()
