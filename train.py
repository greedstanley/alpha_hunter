import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
import numpy as np

# Â∞éÂÖ•ÊàëÂÄëËá™Â∑±ÂØ´ÁöÑÊ®°ÁµÑ
from features.alignment import synthesize_mtf_data
from features.labeling import apply_triple_barrier
from features.preprocess import prepare_features
from models.tcn_core import ParallelTCNAlphaHunter
from utils.loss import FocalLoss, calculate_mcc
from data.dataset import CryptoTimeSeriesDataset

# --- Ë®≠ÂÆöÂèÉÊï∏ (Hyperparameters) ---
CONFIG = {
    'seq_len': 60,  # Benchmark: 30, 40, 60, 80
    'norm_method': 'z_score',  # Benchmark: 'z_score' vs 'log_return'
    'batch_size': 64,
    'epochs': 20,
    'learning_rate': 1e-3,
    'atr_period': 14,
    'horizon': 60,  # Triple Barrier ÁöÑÊôÇÈñìÁâÜ
    'pt_mul': 2.0,  # Ê≠¢ÁõàÂØ¨Â∫¶
    'sl_mul': 2.0,  # Ê≠¢ÊêçÂØ¨Â∫¶
}


def train_model():
    print(f"üöÄ ÂïüÂãï Alpha Hunter Ë®ìÁ∑¥Á®ãÂ∫è...")
    print(f"‚öôÔ∏è  Ë®≠ÂÆö: Seq_Len={CONFIG['seq_len']}, Norm={CONFIG['norm_method']}")

    # 1. ËºâÂÖ•Êï∏Êìö (ÈÄôË£°ÂÅáË®≠‰Ω†Êúâ‰∏ÄÂÄã csvÔºåË´ãÊõøÊèõÊàê‰Ω†ÁöÑÁúüÂØ¶Ë∑ØÂæë)
    # df = pd.read_csv('data/raw/BTCUSDT_1H.csv', index_col='datetime', parse_dates=True)
    # ÁÇ∫‰∫ÜÊºîÁ§∫ÔºåÊàëÂÄëÁîüÊàêÂÅáÊï∏Êìö
    print("‚ö†Ô∏è  ‰ΩøÁî®Èö®Ê©üÂÅáÊï∏ÊìöÈÄ≤Ë°åÊ∏¨Ë©¶ (Ë´ãÊõøÊèõÁÇ∫ÁúüÂØ¶Êï∏Êìö)...")
    dates = pd.date_range(start='2023-01-01', periods=2000, freq='1h')
    df = pd.DataFrame(np.random.random((2000, 5)) * 1000 + 20000,
                      index=dates,
                      columns=['open', 'high', 'low', 'close', 'volume'])

    # 2. ÁâπÂæµÂ∑•Á®ãÁÆ°Á∑ö
    print("üîÑ Âü∑Ë°å Point-in-Time Êï∏ÊìöÂ∞çÈΩä...")
    df_aligned = synthesize_mtf_data(df)

    print("üè∑Ô∏è  ÁîüÊàê Triple Barrier Ê®ôÁ±§...")
    df_labeled = apply_triple_barrier(df_aligned,
                                      horizon=CONFIG['horizon'],
                                      atr_period=CONFIG['atr_period'],
                                      pt_mul=CONFIG['pt_mul'],
                                      sl_mul=CONFIG['sl_mul'])

    print("Scale  Âü∑Ë°åÊï∏ÊìöÊ®ôÊ∫ñÂåñ...")
    df_final = prepare_features(df_labeled,
                                method=CONFIG['norm_method'],
                                window=30)

    # 3. Âª∫Á´ãË≥áÊñôÈõÜËàá DataLoader
    # Á∞°ÂñÆÁöÑÊôÇÈñìÂ∫èÂàóÂàáÂàÜ: Ââç 80% Ë®ìÁ∑¥, Âæå 20% È©óË≠â (‰∏ç‰ΩøÁî®Èö®Ê©üÂàáÂàÜ‰ª•Èò≤ÊºèÈ°å)
    split_idx = int(len(df_final) * 0.8)
    train_df = df_final.iloc[:split_idx]
    val_df = df_final.iloc[split_idx:]

    train_dataset = CryptoTimeSeriesDataset(train_df,
                                            seq_len=CONFIG['seq_len'])
    val_dataset = CryptoTimeSeriesDataset(val_df, seq_len=CONFIG['seq_len'])

    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG['batch_size'],
        shuffle=True)  # Ë®ìÁ∑¥ÈõÜÂèØ‰ª• Shuffle batchÔºåÂõ†ÁÇ∫È†ÜÂ∫èÂú® Dataset ÂÖßÈÉ®Â∑≤Á∂ì‰øùÁïô
    val_loader = DataLoader(val_dataset,
                            batch_size=CONFIG['batch_size'],
                            shuffle=False)

    # 4. ÂàùÂßãÂåñÊ®°Âûã
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ParallelTCNAlphaHunter(input_features=5, num_classes=3).to(device)

    # 5. Ë®≠ÂÆö Loss Ëàá Optimizer
    # alpha: Ë®≠ÂÆöÈ°ûÂà•Ê¨äÈáç (Ëß£Ê±∫‰∏çÂπ≥Ë°°), gamma: Â∞àÊ≥®Èõ£Ê®£Êú¨
    # ÂÅáË®≠È°ûÂà•ÂàÜ‰Ωà: Hold(0): 70%, Buy(1): 15%, Sell(2): 15% -> Ê¨äÈáçË®≠ÁÇ∫ [0.3, 1.0, 1.0]
    focal_loss = FocalLoss(alpha=torch.tensor([0.3, 1.0, 1.0]).to(device),
                           gamma=2.0)
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])

    # 6. Ë®ìÁ∑¥Ëø¥Âúà
    for epoch in range(CONFIG['epochs']):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []

        for batch in train_loader:
            x_1h = batch['1h'].to(device)
            x_4h = batch['4h'].to(device)
            x_1d = batch['1d'].to(device)
            y = batch['label'].to(device)

            optimizer.zero_grad()
            logits = model(x_1h, x_4h, x_1d)
            loss = focal_loss(logits, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_preds.append(logits.detach())
            train_targets.append(y)

        # Ë®àÁÆóË®ìÁ∑¥ÈõÜ MCC
        train_preds = torch.cat(train_preds)
        train_targets = torch.cat(train_targets)
        train_mcc = calculate_mcc(train_preds, train_targets)

        # È©óË≠â
        model.eval()
        val_preds, val_targets = [], []
        with torch.no_grad():
            for batch in val_loader:
                x_1h = batch['1h'].to(device)
                x_4h = batch['4h'].to(device)
                x_1d = batch['1d'].to(device)
                y = batch['label'].to(device)

                logits = model(x_1h, x_4h, x_1d)
                val_preds.append(logits)
                val_targets.append(y)

        val_preds = torch.cat(val_preds)
        val_targets = torch.cat(val_targets)
        val_mcc = calculate_mcc(val_preds, val_targets)

        print(
            f"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {train_loss/len(train_loader):.4f} | Train MCC: {train_mcc:.3f} | Val MCC: {val_mcc:.3f}"
        )


if __name__ == "__main__":
    train_model()
