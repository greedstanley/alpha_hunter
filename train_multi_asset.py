import torch
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset # å¼•å…¥ ConcatDataset
import pandas as pd
import numpy as np
import os

# å°Žå…¥æ¨¡çµ„
from features.alignment import synthesize_mtf_data
from features.labeling import apply_triple_barrier
from features.preprocess import prepare_features
from models.tcn_core import ParallelTCNAlphaHunter
from utils.loss import FocalLoss, calculate_mcc
from data.dataset import CryptoTimeSeriesDataset
from train import load_and_clean_data, CONFIG # è¤‡ç”¨è¨­å®š

def process_single_asset(filepath):
    """
    å°å–®ä¸€å¹£ç¨®é€²è¡Œå®Œæ•´çš„ç‰¹å¾µå·¥ç¨‹æµç¨‹
    """
    if not os.path.exists(filepath):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {filepath}ï¼Œè·³éŽã€‚")
        return None, None

    df = load_and_clean_data(filepath)
    
    # 1. å°é½Š (åˆæˆ 4H/1D)
    df_aligned = synthesize_mtf_data(df)
    
    # 2. æ¨™ç±¤ (Triple Barrier)
    # æ³¨æ„ï¼šATR æœƒè‡ªå‹•é©æ‡‰ä¸åŒå¹£ç¨®çš„åƒ¹æ ¼ scaleï¼Œæ‰€ä»¥é€™è£¡åƒæ•¸ä¸ç”¨æ”¹
    df_labeled = apply_triple_barrier(df_aligned, horizon=CONFIG['horizon'], atr_period=CONFIG['atr_period'])
    
    # 3. æ¨™æº–åŒ– (Z-Score/LogReturn)
    # é€™æ˜¯é—œéµï¼å› ç‚ºåšäº†æ¨™æº–åŒ–ï¼ŒBTC å’Œ ETH çš„æ•¸å€¼åˆ†ä½ˆæœƒè®Šå¾—ä¸€æ¨£ï¼Œå¯ä»¥æ··åˆè¨“ç·´
    df_final = prepare_features(df_labeled, method=CONFIG['norm_method'], window=30)
    df_final = df_final.dropna()
    
    # 4. åˆ‡åˆ†è¨“ç·´/é©—è­‰
    split_idx = int(len(df_final) * 0.8)
    train_df = df_final.iloc[:split_idx]
    val_df = df_final.iloc[split_idx:]
    
    return train_df, val_df

def train_multi_asset_model():
    print(f"ðŸš€ å•Ÿå‹• Alpha Hunter [å¤šå¹£ç¨®] è¨“ç·´ç¨‹åº...")
    print(f"âš™ï¸  Epochs={CONFIG['epochs']}, Norm={CONFIG['norm_method']}")
    
    # --- å®šç¾©è¦è¨“ç·´çš„å¹£ç¨®æ¸…å–® ---
    # è«‹ç¢ºä¿ data/raw/ è³‡æ–™å¤¾ä¸‹æœ‰é€™äº›æª”æ¡ˆ
    asset_files = [
        os.path.join('data', 'raw', 'BTCUSDT_1H.csv'),
        os.path.join('data', 'raw', 'ETHUSDT_1H.csv'),
        os.path.join('data', 'raw', 'BNBUSDT_1H.csv'),
        os.path.join('data', 'raw', 'SOLUSDT_1H.csv'),
        # ä½ ä¹‹å¾Œå¯ä»¥ä¸‹è¼‰ SOLUSDT, BNBUSDT ç­‰åŠ é€²ä¾†
    ]
    
    train_datasets = []
    val_datasets = []
    
    for filepath in asset_files:
        print(f"\nðŸ”„ è™•ç†è³‡ç”¢: {os.path.basename(filepath)} ...")
        t_df, v_df = process_single_asset(filepath)
        
        if t_df is not None:
            # ç‚ºæ¯å€‹å¹£ç¨®å»ºç«‹ç¨ç«‹çš„ Dataset (ç¢ºä¿æ™‚é–“åºåˆ—ä¸ä¸­æ–·)
            train_datasets.append(CryptoTimeSeriesDataset(t_df, seq_len=CONFIG['seq_len']))
            val_datasets.append(CryptoTimeSeriesDataset(v_df, seq_len=CONFIG['seq_len']))
            print(f"   Samples -> Train: {len(t_df)}, Val: {len(v_df)}")
            
    if not train_datasets:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„è¨“ç·´æ•¸æ“šï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        return

    # --- é—œéµä¸€æ­¥ï¼šåˆä½µæ•¸æ“šé›† ---
    # ConcatDataset æœƒæŠŠå¤šå€‹ Dataset è™›æ“¬åœ°æŽ¥åœ¨ä¸€èµ·ï¼Œè®“ DataLoader ä»¥ç‚ºé€™æ˜¯ä¸€å€‹å¤§è³‡æ–™åº«
    combined_train_dataset = ConcatDataset(train_datasets)
    combined_val_dataset = ConcatDataset(val_datasets)
    
    print(f"\nðŸ“Š [ç¸½è¨ˆ] è¨“ç·´æ¨£æœ¬æ•¸: {len(combined_train_dataset)}, é©—è­‰æ¨£æœ¬æ•¸: {len(combined_val_dataset)}")
    
    # å»ºç«‹ DataLoader (æ··åˆäº†æ‰€æœ‰å¹£ç¨®çš„æ•¸æ“š)
    train_loader = DataLoader(combined_train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    val_loader = DataLoader(combined_val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
    
    # --- ä»¥ä¸‹æ¨¡åž‹åˆå§‹åŒ–èˆ‡è¨“ç·´é‚è¼¯èˆ‡åŽŸæœ¬ç›¸åŒ ---
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸ’» ä½¿ç”¨è£ç½®: {device}")
    
    model = ParallelTCNAlphaHunter(input_features=5, num_classes=3).to(device)
    focal_loss = FocalLoss(alpha=torch.tensor([0.5, 1.0, 1.0]).to(device), gamma=2.0)
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
    
    checkpoint_dir = os.path.join('models', 'checkpoints')
    if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)
    best_val_mcc = -float('inf')

    # ... (é€™è£¡çœç•¥é‡è¤‡çš„ Resume é‚è¼¯ï¼Œèˆ‡ train.py ç›¸åŒ) ...
    # ç‚ºäº†ç°¡æ½”ï¼Œé€™è£¡ç›´æŽ¥é–‹å§‹è¨“ç·´ loop

    for epoch in range(CONFIG['epochs']):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []
        
        for batch in train_loader:
            x_1h, x_4h, x_1d, y = batch['1h'].to(device), batch['4h'].to(device), batch['1d'].to(device), batch['label'].to(device)
            optimizer.zero_grad()
            logits = model(x_1h, x_4h, x_1d)
            loss = focal_loss(logits, y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.append(logits.detach())
            train_targets.append(y)
            
        train_mcc = calculate_mcc(torch.cat(train_preds), torch.cat(train_targets)) if train_preds else 0
        
        # é©—è­‰
        model.eval()
        val_preds, val_targets = [], []
        with torch.no_grad():
            for batch in val_loader:
                x_1h, x_4h, x_1d, y = batch['1h'].to(device), batch['4h'].to(device), batch['1d'].to(device), batch['label'].to(device)
                logits = model(x_1h, x_4h, x_1d)
                val_preds.append(logits)
                val_targets.append(y)
        
        val_mcc = calculate_mcc(torch.cat(val_preds), torch.cat(val_targets)) if val_preds else 0
        
        print(f"Epoch {epoch+1}/{CONFIG['epochs']} | Loss: {train_loss/len(train_loader):.4f} | Train MCC: {train_mcc:.3f} | Val MCC: {val_mcc:.3f}")
        
        if val_mcc > best_val_mcc:
            best_val_mcc = val_mcc
            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))
            print(f"    ðŸ”¥ æ–°é«˜é»ž (MCC: {val_mcc:.3f}) -> æ¨¡åž‹å·²æ›´æ–°")

if __name__ == "__main__":
    train_multi_asset_model()