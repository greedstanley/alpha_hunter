import torch
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset
import pandas as pd
import numpy as np
import os
import glob

# å°å…¥æ¨¡çµ„
from features.alignment import synthesize_mtf_data
from features.labeling import apply_triple_barrier
from features.preprocess import prepare_features
from models.tcn_core import ParallelTCNAlphaHunter
from utils.loss import FocalLoss, calculate_mcc
from data.dataset import CryptoTimeSeriesDataset
from train import load_and_clean_data  # è¤‡ç”¨å‡½æ•¸

# é‡æ–°å®šç¾© CONFIG ä»¥ä¾¿æ–¼æ­¤æª”æ¡ˆç¨ç«‹é‹è¡Œ
CONFIG = {
    'seq_len': 60,
    'norm_method': 'z_score',
    'batch_size': 64,
    'epochs': 20,          # é€™æ˜¯é è¨­çš„ã€Œå–®æ¬¡ã€è¨“ç·´ç›®æ¨™
    'learning_rate': 1e-3,
    'atr_period': 14,
    'horizon': 60,
    'pt_mul': 2.0,
    'sl_mul': 2.0,
}

def process_single_asset(filepath):
    """è™•ç†å–®ä¸€å¹£ç¨®æ•¸æ“š"""
    if not os.path.exists(filepath):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {filepath}ï¼Œè·³éã€‚")
        return None, None

    df = load_and_clean_data(filepath)
    df_aligned = synthesize_mtf_data(df)
    # é€™è£¡çš„ horizon ç”¨æ–¼æ¨™ç±¤ç”Ÿæˆï¼Œè·Ÿå›æ¸¬ç„¡é—œ
    df_labeled = apply_triple_barrier(df_aligned, horizon=CONFIG['horizon'], atr_period=CONFIG['atr_period'])
    df_final = prepare_features(df_labeled, method=CONFIG['norm_method'], window=30)
    df_final = df_final.dropna()
    
    # ç°¡å–®çš„æ™‚é–“åˆ‡åˆ†
    split_idx = int(len(df_final) * 0.8)
    train_df = df_final.iloc[:split_idx]
    val_df = df_final.iloc[split_idx:]
    
    return train_df, val_df

def save_checkpoint(model, optimizer, epoch, val_mcc, filename):
    """ä¿å­˜å®Œæ•´çš„è¨“ç·´ç‹€æ…‹"""
    state = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'val_mcc': val_mcc,
        'config': CONFIG
    }
    torch.save(state, filename)
    print(f"    ğŸ’¾ Checkpoint saved: {filename} (MCC: {val_mcc:.4f})")

def train_multi_asset_model(resume=False, additional_epochs=0):
    """
    Args:
        resume (bool): æ˜¯å¦å¾ best_model.pth æ¢å¾©è¨“ç·´
        additional_epochs (int): å¦‚æœæ˜¯ resumeï¼Œè¦é¡å¤–å†è¨“ç·´å¤šå°‘å€‹ epochs
    """
    print(f"ğŸš€ å•Ÿå‹• Alpha Hunter [å¤šå¹£ç¨®] è¨“ç·´ç¨‹åº...")
    
    # 1. æº–å‚™æ•¸æ“š
    # æœå°‹ data/raw ä¸‹æ‰€æœ‰çš„ _1H.csv æª”æ¡ˆ
    asset_files = glob.glob(os.path.join('data', 'raw', '*_1H.csv'))
    if not asset_files:
        # Fallback for explicit list if glob fails or folder structure differs
        asset_files = [
            os.path.join('data', 'raw', 'BTCUSDT_1H.csv'),
            os.path.join('data', 'raw', 'ETHUSDT_1H.csv'),
        ]
    
    print(f"ğŸ“‹ åµæ¸¬åˆ°è³‡ç”¢æª”æ¡ˆ: {[os.path.basename(f) for f in asset_files]}")

    train_datasets = []
    val_datasets = []
    
    for filepath in asset_files:
        t_df, v_df = process_single_asset(filepath)
        if t_df is not None and len(t_df) > CONFIG['seq_len']:
            train_datasets.append(CryptoTimeSeriesDataset(t_df, seq_len=CONFIG['seq_len']))
            val_datasets.append(CryptoTimeSeriesDataset(v_df, seq_len=CONFIG['seq_len']))
            
    if not train_datasets:
        print("âŒ ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œçµ‚æ­¢ã€‚")
        return

    combined_train = ConcatDataset(train_datasets)
    combined_val = ConcatDataset(val_datasets)
    
    train_loader = DataLoader(combined_train, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0) # Windows/Colabæœ‰æ—¶è®¾ num_workers=0 æ›´ç¨³
    val_loader = DataLoader(combined_val, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š ç¸½è¨“ç·´æ¨£æœ¬: {len(combined_train)} | ç¸½é©—è­‰æ¨£æœ¬: {len(combined_val)}")

    # 2. åˆå§‹åŒ–æ¨¡å‹èˆ‡ç’°å¢ƒ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» Device: {device}")
    
    model = ParallelTCNAlphaHunter(input_features=5, num_classes=3).to(device)
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
    focal_loss = FocalLoss(alpha=torch.tensor([0.5, 1.0, 1.0]).to(device), gamma=2.0)
    
    checkpoint_dir = os.path.join('models', 'checkpoints')
    if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)
    
    best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
    
    # 3. æ–·é»çºŒè¨“é‚è¼¯
    start_epoch = 0
    best_val_mcc = -1.0 # Initialize low
    
    if resume and os.path.exists(best_model_path):
        print(f"ğŸ”„ è¼‰å…¥ Checkpoint: {best_model_path}")
        checkpoint = torch.load(best_model_path, map_location=device)
        
        # å…¼å®¹æ€§æª¢æŸ¥ï¼šç¢ºèª checkpoint æ ¼å¼
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_mcc = checkpoint.get('val_mcc', 0.0)
            print(f"   âœ… æˆåŠŸæ¢å¾©ç‹€æ…‹ã€‚ä¸Šæ¬¡åœæ­¢æ–¼ Epoch {checkpoint['epoch']}, Best MCC: {best_val_mcc:.4f}")
        else:
            # èˆŠç‰ˆåªæœ‰ state_dict çš„æƒ…æ³
            model.load_state_dict(checkpoint)
            print("   âš ï¸ åƒ…è¼‰å…¥æ¬Šé‡ (èˆŠç‰ˆæ ¼å¼)ï¼ŒOptimizer ç‹€æ…‹å·²é‡ç½®ã€‚")
            
    # è¨­å®šç¸½ç›®æ¨™ Epochs
    total_epochs = CONFIG['epochs']
    if resume:
        total_epochs = start_epoch + additional_epochs
        print(f"ğŸ¯ çºŒè¨“æ¨¡å¼: ç›®æ¨™å¾ Epoch {start_epoch} ç·´åˆ° {total_epochs}")
    else:
        print(f"ğŸ¯ å…¨æ–°è¨“ç·´: ç›®æ¨™ {total_epochs} Epochs")

    # 4. è¨“ç·´è¿´åœˆ
    for epoch in range(start_epoch, total_epochs):
        model.train()
        train_loss = 0
        train_preds, train_targets = [], []
        
        for batch in train_loader:
            x_1h = batch['1h'].to(device)
            x_4h = batch['4h'].to(device)
            x_1d = batch['1d'].to(device)
            y = batch['label'].to(device)
            
            optimizer.zero_grad()
            logits = model(x_1h, x_4h, x_1d)
            loss = focal_loss(logits, y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.append(logits.detach())
            train_targets.append(y)
            
        # å¿«é€Ÿè¨ˆç®— Train MCC (ä½¿ç”¨ GPU tensor é‹ç®—é¿å… CPU copy é–‹éŠ·)
        train_loss_avg = train_loss / len(train_loader)
        
        # Validation
        model.eval()
        val_preds, val_targets = [], []
        with torch.no_grad():
            for batch in val_loader:
                x_1h, x_4h, x_1d, y = batch['1h'].to(device), batch['4h'].to(device), batch['1d'].to(device), batch['label'].to(device)
                logits = model(x_1h, x_4h, x_1d)
                val_preds.append(logits)
                val_targets.append(y)
        
        if val_preds:
            val_all_preds = torch.cat(val_preds)
            val_all_targets = torch.cat(val_targets)
            val_mcc = calculate_mcc(val_all_preds, val_all_targets)
        else:
            val_mcc = 0.0

        # å› ç‚º calculate_mcc å¯èƒ½éœ€è¦ CPUï¼Œé€™è£¡ç°¡åŒ– log
        print(f"Epoch {epoch+1}/{total_epochs} | Loss: {train_loss_avg:.4f} | Val MCC: {val_mcc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_mcc > best_val_mcc:
            best_val_mcc = val_mcc
            save_checkpoint(model, optimizer, epoch, val_mcc, best_model_path)

if __name__ == "__main__":
    # ä½¿ç”¨ç¯„ä¾‹ï¼š
    # 1. å…¨æ–°è¨“ç·´ 20 epochs
    # train_multi_asset_model(resume=False)
    
    # 2. çºŒè¨“ï¼šå‡è¨­ä¹‹å‰è·‘äº† 20ï¼Œç¾åœ¨æƒ³å†åŠ  50 (ç¸½å…±åˆ° 70)
    train_multi_asset_model(resume=True, additional_epochs=50)